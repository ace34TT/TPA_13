# Big Data Pipeline Project

Welcome to the Big Data Pipeline Project! This project is designed to showcase a comprehensive big data processing workflow, running entirely within a Vagrant virtual machine (VM). It encompasses various stages of data handling and processing, from data ingestion to machine learning and web-based visualization.

## Key Features

1. **Data Source Feeding**: Initial data collection and ingestion from multiple sources into the pipeline.
2. **MapReduce Task**: Distributed processing of large datasets using the MapReduce paradigm.
3. **Extract, Load, Transform (ELT)**: Efficient transformation and loading of data into a structured format suitable for analysis.
4. **Data Lake Ingesting**: Storing and managing large volumes of structured and unstructured data in a data lake.
5. **Machine Learning**: Implementing machine learning models to derive insights and predictions from the processed data.
6. **Web Service**: Exposing data and machine learning results via a RESTful web service.
7. **Web Application**: An interactive web application for visualizing data and insights generated by the pipeline.

## Project Overview

This project aims to provide a robust and scalable solution for handling big data, leveraging modern data processing technologies and frameworks. By running the entire pipeline within a Vagrant VM, the project ensures a consistent and reproducible environment, simplifying the setup and deployment processes.

## Requirements

To run the Big Data Pipeline Project, you will need to ensure Apache Airflow is installed within the Vagrant virtual machine (VM). The VM setup instructions are provided in a separate README file located at `others/vm_readme.md`.

### Apache Airflow (installed within the VM)
- **Python**: Version 3.9 
- **Apache Airflow**: Version 2.9.1

After provisioning the VM, you must install Apache Airflow within the VM environment.

If you need assistance with installing Apache Airflow within the VM, please refer to the official Apache Airflow documentation or follow these general steps:

### Installing Apache Airflow

1. **SSH into the Vagrant VM**:
    ```bash
    vagrant ssh
    ```

2. **Install Apache Airflow**:
    ```bash
    pip install apache-airflow 
    ```

3. **Initialize Airflow**:
    ```bash
    airflow db init
    ```

4. **Create an Airflow Admin User**:
    ```bash
    airflow users create \
        --username admin \
        --password admin \
        --firstname Admin \
        --lastname User \
        --role Admin \
        --email admin@example.com
    ```

5. **Port Forwarding**: Before starting Airflow, add port forwarding to your Vagrantfile to ensure port 8080 on the VM is forwarded to your host machine:
    ```ruby
    config.vm.network "forwarded_port", guest: 8080, host: 8080
    ```

6. **Start Airflow Scheduler**:
    ```bash
    airflow scheduler
    ```

7.  **Start Airflow Web Server** (in a new terminal):
    ```bash
    airflow webserver --port 8080
    ```

Once Airflow is installed, you can access the Airflow web interface at `http://localhost:8080` on your host machine. Log in using the admin credentials you set up in step 4 (`username: admin`, `password: admin`). This completes the setup of the missing component in the VM.


## Setup Instructions

Follow these steps to set up and run the Big Data Pipeline Project:

1. **Clone the Repository**: Clone the project repository to your Vagrant VM's root directory:
    ```bash
    git clone https://github.com/ace34TT/tpa_13.git
    ```

2. **Copy Airflow Dags** : Copy all files from the `/vagrant/big-data-pipeline/dags` directory to the `~/airflow/dags` directory within the VM:
    ```bash
    cp -r /vagrant/tpa_13/dags/*.py ~/airflow/dags/
    ```
3. **Create Hive Database** : run beeline to connect to hive  `beeline` 
   ```sql
    -- connect to hive
    !connect jdbc:hive2://localhost:10000/
    -- use empty username and empty password
    -- create a database 
    CREATE DATABASE tpa_13 ;
   ```
4. **Create Oracle sql User** :
    ```sql
    -- connect to oracle database 
    sudo -su oracle
    sqlplus /nolog

    -- se connecter avec compte system pour créer un utilisateur mbds
    connect system@ORCLPDB/Welcome1

    -- création de l'utilisateur MBDS
    create user MBDS identified by PassMbds
    default tablespace users
    temporary tablespace temp;

    grant dba to MBDS;

    -- ALTER USER MBDS QUOTA UNLIMITED ON USERS;
    revoke unlimited tablespace from MBDS;
    ```
5. **Install R packages**
To install all R required packages run the `packages.r` 
```bash
cd /vagrant/tpa_13/scripts/3_data_analysis/ &&
Rscript packages.r
```

## Usage
### Data Source and Data lake
#### Verify DAGs
In the Airflow UI, navigate to the DAGs section. You should see the following DAGs listed:
- `1_data_source`
- `2_map_reduce`
- `3_elt`
- `4_data_lake_ingestion`
- `big_data_pipeline`

Enable and trigger the DAGs as required.
#### DAG Overview

##### Data Source Ingestion (1_data_source.py)
This DAG handles the initial ingestion of data from various sources.

##### Map-Reduce Operations (2_map_reduce.py)
This DAG performs map-reduce operations on the ingested data to process and transform it.

##### ELT Process (3_elt.py)
This DAG executes the ELT processes, transforming the raw data into a structured format suitable for analysis.

##### Data Lake Ingestion (4_data_lake_ingestion.py)
This DAG ingests the processed data into a data lake for long-term storage and further analysis.

##### Big Data Pipeline (big_data_pipeline.py)

This DAG orchestrates the entire pipeline, coordinating the execution of the above steps in sequence.

## Data Analysis

The data analysis phase involves generating a machine learning model and using it to process marketing data. Below are the detailed steps to execute these processes effectively.

> ⚠️ **Warning**: Although the process can be run within the virtual machine (VM), it is recommended to run it on your local machine due to limited resources in the VM. This will ensure faster execution and prevent resource exhaustion.

### Model Generation

To generate the machine learning model, follow these steps:

1. **Navigate to the Analysis Scripts Folder**:
   Move to the directory containing the R scripts for data analysis:
   ```bash
   cd /vagrant/tpa_13/scripts/3_data_analysis/
   ```

2. **Install Required Packages and Run the Training Script**:
   Execute the `packages.r` script to install the necessary R packages and run the training process:
   ```bash
   Rscript packages.r
   ```

   This script will process your data and create a machine learning model. Upon completion, a model file named `categorie_model.rds` will be generated and saved in the `scripts/3_data_analysis/models/` directory.

### Marketing Processing

After generating the model, you can use it to process marketing data. The results will be stored in an Oracle SQL database. Follow these steps:

1. **Navigate to the Result Database Scripts Folder**:
   Move to the directory containing the script for marketing data processing:
   ```bash
   cd /vagrant/tpa_13/scripts/4_result_database/
   ```

2. **Run the Marketing Processing Script**:
   Execute the `marketing.r` script to process the marketing data using the generated model:
   ```bash
   Rscript marketing.r
   ```

   This script will apply the machine learning model to the marketing data and save the processed results into an Oracle SQL database.

## Web Application

The web application provides an API and a frontend interface for interacting with the processed data and results.

### API

To run the web service API, follow these steps:

1. **Navigate to the API Scripts Folder**:
   Move to the directory containing the R script for the web service:
   ```bash
   cd /vagrant/tpa_13/web_app/tpa_13_api/
   ```

2. **Start the API Service**:
   Execute the `plumber.R` script to start the web service:
   ```bash
   Rscript plumber.R
   ```

   This script uses the Plumber package to create an API that serves the processed data. The API will be available for consumption by other applications or services.

### Web Frontend

To access the web frontend, follow these steps:

1. **Open the Frontend in Your Browser**:
   Navigate to the frontend directory and open the `index.html` file in your browser:
   ```bash
   open /vagrant/tpa_13/web_app/frontend/src/index.html
   ```

   This file provides a graphical user interface (GUI) for interacting with the API and viewing the processed data. The frontend is built using standard web technologies and can be accessed locally on your machine.

By following these detailed instructions, you can efficiently run the data analysis, generate the machine learning model, process marketing data, and deploy the web application.
