# Big Data Pipeline Project

Welcome to the Big Data Pipeline Project! This project is designed to showcase a comprehensive big data processing workflow, running entirely within a Vagrant virtual machine (VM). It encompasses various stages of data handling and processing, from data ingestion to machine learning and web-based visualization.

## Key Features

1. **Data Source Feeding**: Initial data collection and ingestion from multiple sources into the pipeline.
2. **MapReduce Task**: Distributed processing of large datasets using the MapReduce paradigm.
3. **Extract, Load, Transform (ELT)**: Efficient transformation and loading of data into a structured format suitable for analysis.
4. **Data Lake Ingesting**: Storing and managing large volumes of structured and unstructured data in a data lake.
5. **Machine Learning**: Implementing machine learning models to derive insights and predictions from the processed data.
6. **Web Service**: Exposing data and machine learning results via a RESTful web service.
7. **Web Application**: An interactive web application for visualizing data and insights generated by the pipeline.

## Project Overview

This project aims to provide a robust and scalable solution for handling big data, leveraging modern data processing technologies and frameworks. By running the entire pipeline within a Vagrant VM, the project ensures a consistent and reproducible environment, simplifying the setup and deployment processes.

## Requirements

To run the Big Data Pipeline Project, you will need to ensure Apache Airflow is installed within the Vagrant virtual machine (VM). The VM setup instructions are provided in a separate README file located at `others/vm_readme.md`.

### Apache Airflow (installed within the VM)
- **Python**: Version 3.9 
- **Apache Airflow**: Version 2.9.1

After provisioning the VM, you must install Apache Airflow within the VM environment.

If you need assistance with installing Apache Airflow within the VM, please refer to the official Apache Airflow documentation or follow these general steps:

### Installing Apache Airflow

1. **SSH into the Vagrant VM**:
    ```bash
    vagrant ssh
    ```

2. **Install Apache Airflow**:
    ```bash
    pip install apache-airflow 
    ```

3. **Initialize Airflow**:
    ```bash
    airflow db init
    ```

4. **Create an Airflow Admin User**:
    ```bash
    airflow users create \
        --username admin \
        --password admin \
        --firstname Admin \
        --lastname User \
        --role Admin \
        --email admin@example.com
    ```

5. **Port Forwarding**: Before starting Airflow, add port forwarding to your Vagrantfile to ensure port 8080 on the VM is forwarded to your host machine:
    ```ruby
    config.vm.network "forwarded_port", guest: 8080, host: 8080
    ```

6. **Start Airflow Scheduler**:
    ```bash
    airflow scheduler
    ```

7.  **Start Airflow Web Server** (in a new terminal):
    ```bash
    airflow webserver --port 8080
    ```

Once Airflow is installed, you can access the Airflow web interface at `http://localhost:8080` on your host machine. Log in using the admin credentials you set up in step 4 (`username: admin`, `password: admin`). This completes the setup of the missing component in the VM.


## Setup Instructions

Follow these steps to set up and run the Big Data Pipeline Project:

1. **Clone the Repository**: Clone the project repository to your Vagrant VM's root directory:
    ```bash
    git clone https://github.com/ace34TT/tpa_13.git
    ```

2. **Copy Airflow Dags** : Copy all files from the `/vagrant/big-data-pipeline/dags` directory to the `~/airflow/dags` directory within the VM:
    ```bash
    cp -r /vagrant/tpa_13/dags/*.py ~/airflow/dags/
    ```
3. **Create Hive Database** : run beeline to connect to hive  `beeline` 
   ```sql
    -- connect to hive
    !connect jdbc:hive2://localhost:10000/
    -- use empty username and empty password
    -- create a database 
    CREATE DATABASE tpa_13 ;
   ```
4. **Create Oracle sql User** :
    ```sql
    -- connect to oracle database 
    sudo -su oracle
    sqlplus /nolog

    -- se connecter avec compte system pour créer un utilisateur mbds
    connect system@ORCLPDB/Welcome1

    -- création de l'utilisateur MBDS
    create user MBDS identified by PassMbds
    default tablespace users
    temporary tablespace temp;

    grant dba to MBDS;

    -- ALTER USER MBDS QUOTA UNLIMITED ON USERS;
    revoke unlimited tablespace from MBDS;
    ```

## Walkthrough

### Data Source Feeding

Use the folDFS

Run the script to import data into HDFS:

```bash
sh /vagrant/tpa_13/scripts/1_data_source/hdfs/hdfs_import.sh
```

#### MongoDB

Run the script to import data into MongoDB:

```bash
sh /vagrant/tpa_13/scripts/1_data_source/mongo_db.sh
```

#### Oracle NoSQL

Run the script to import data into Oracle NoSQL:

```bash
java -jar /vagrant/tpa_13/scripts/1_data_source/oracle_nosql/marketing_to_oracle_nosql.jar
```

### MapReduce Job

Execute the MapReduce job with the following script:

```bashlowing scripts to feed data into the respective sources.

#### H
sh /vagrant/tpa_13/scripts/1_data_source/map_reduce/map_reduce_catalog_co2.sh
```

### Data Lake Operations

#### Table Creation

Run the script to initialize tables in the data lake:

```bash
python3.9 /vagrant/tpa_13/scripts/2_data_lake/table_init.py
```

#### Extract, Load, Transform (ELT)

Run the script to perform ELT operations:

```bash
python3.9 /vagrant/tpa_13/scripts/2_data_lake/clients_elt.py
```

## Additional Notes

Ensure all scripts are executed with the correct permissions. Use `chmod +x <script>` to make a script executable if needed. Verify that all necessary dependencies and services are properly configured and running before executing the scripts.